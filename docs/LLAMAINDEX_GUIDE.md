# LlamaIndex ê³„ì¸µì  ì¸ë±ì‹± ì™„ë²½ ê°€ì´ë“œ

LlamaIndexë¥¼ í™œìš©í•˜ì—¬ ë³µì¡í•œ ë¬¸ì„œ êµ¬ì¡°(Table, JSON)ë¥¼ ê³„ì¸µì ìœ¼ë¡œ ì¸ë±ì‹±í•˜ëŠ” ë°©ë²•ì„ í•™ìŠµí•©ë‹ˆë‹¤.

---

## ğŸ“š ëª©ì°¨

1. [LlamaIndexë€?](#1-llamaindexë€)
2. [ì™œ ê³„ì¸µì  ì¸ë±ì‹±ì¸ê°€?](#2-ì™œ-ê³„ì¸µì -ì¸ë±ì‹±ì¸ê°€)
3. [í•µì‹¬ ê°œë…](#3-í•µì‹¬-ê°œë…)
4. [ì¸ë±ìŠ¤ íƒ€ì…](#4-ì¸ë±ìŠ¤-íƒ€ì…)
5. [ì‹¤ìŠµ ì˜ˆì œ](#5-ì‹¤ìŠµ-ì˜ˆì œ)
6. [FastAPI ì—”ë“œí¬ì¸íŠ¸](#6-fastapi-ì—”ë“œí¬ì¸íŠ¸)
7. [ê³ ê¸‰ íŒ¨í„´](#7-ê³ ê¸‰-íŒ¨í„´)
8. [ì„±ëŠ¥ ìµœì í™”](#8-ì„±ëŠ¥-ìµœì í™”)
9. [ë¬¸ì œ í•´ê²°](#9-ë¬¸ì œ-í•´ê²°)

---

## 1. LlamaIndexë€?

**LlamaIndex**ëŠ” LLM ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ìœ„í•œ ë°ì´í„° í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.

### ì£¼ìš” ê¸°ëŠ¥

- **ë°ì´í„° ì—°ê²°**: ë‹¤ì–‘í•œ ì†ŒìŠ¤(íŒŒì¼, DB, API)ì—ì„œ ë°ì´í„° ë¡œë“œ
- **ë°ì´í„° ì¸ë±ì‹±**: íš¨ìœ¨ì ì¸ ê²€ìƒ‰ì„ ìœ„í•œ êµ¬ì¡°í™”
- **ì¿¼ë¦¬ ì—”ì§„**: ìì—°ì–´ ì§ˆì˜ ì²˜ë¦¬
- **RAG (Retrieval Augmented Generation)**: ê²€ìƒ‰ ê¸°ë°˜ ìƒì„±

### LangChain vs LlamaIndex

| ê¸°ëŠ¥ | LangChain | LlamaIndex |
|-----|-----------|-----------|
| ì£¼ ëª©ì  | ì²´ì¸ êµ¬ì„±, ì›Œí¬í”Œë¡œìš° | ë°ì´í„° ì¸ë±ì‹±, ê²€ìƒ‰ |
| ê°•ì  | ìœ ì—°í•œ íŒŒì´í”„ë¼ì¸ | ê³„ì¸µì  ì¸ë±ì‹± |
| ì‚¬ìš© ì‚¬ë¡€ | ë³µì¡í•œ LLM ì›Œí¬í”Œë¡œìš° | RAG, ë¬¸ì„œ ê²€ìƒ‰ |

---

## 2. ì™œ ê³„ì¸µì  ì¸ë±ì‹±ì¸ê°€?

### ë¬¸ì œì : Flat Indexingì˜ í•œê³„

```
ì „ì²´ ë¬¸ì„œ (10,000 ë‹¨ì–´)
  â†’ í•˜ë‚˜ì˜ í° ì„ë² ë”©
  â†’ ê²€ìƒ‰ ì‹œ ì •í™•ë„ ë‚®ìŒ
  â†’ ì»¨í…ìŠ¤íŠ¸ ì†ì‹¤
```

### í•´ê²°ì±…: ê³„ì¸µì  ì¸ë±ì‹±

```
ì „ì²´ ë¬¸ì„œ
  â”œâ”€â”€ ì„¹ì…˜ 1 (Parent Node)
  â”‚   â”œâ”€â”€ ì²­í¬ 1-1 (Child Node)
  â”‚   â”œâ”€â”€ ì²­í¬ 1-2
  â”‚   â””â”€â”€ ì²­í¬ 1-3
  â”œâ”€â”€ ì„¹ì…˜ 2 (Parent Node)
  â”‚   â”œâ”€â”€ ì²­í¬ 2-1
  â”‚   â””â”€â”€ ì²­í¬ 2-2
```

### ì¥ì 

1. **ì •í™•í•œ ê²€ìƒ‰**: ì‘ì€ ì²­í¬ë¡œ ì„¸ë°€í•œ ë§¤ì¹­
2. **í’ë¶€í•œ ì»¨í…ìŠ¤íŠ¸**: í•„ìš” ì‹œ ë¶€ëª¨ ë…¸ë“œ ì°¸ì¡°
3. **íš¨ìœ¨ì ì¸ ë©”ëª¨ë¦¬**: í•„ìš”í•œ ë¶€ë¶„ë§Œ ë¡œë“œ
4. **ìœ ì—°í•œ ì¿¼ë¦¬**: ì—¬ëŸ¬ ë ˆë²¨ì—ì„œ ê²€ìƒ‰ ê°€ëŠ¥

---

## 3. í•µì‹¬ ê°œë…

### Document

ë¬¸ì„œì˜ ê¸°ë³¸ ë‹¨ìœ„ì…ë‹ˆë‹¤.

```python
from llama_index.core import Document

doc = Document(
    text="LlamaIndex is awesome!",
    metadata={"source": "blog", "date": "2024-01-01"}
)
```

### Node

ì¸ë±ì‹±ì˜ ìµœì†Œ ë‹¨ìœ„ì…ë‹ˆë‹¤.

```python
from llama_index.core.schema import TextNode

node = TextNode(
    text="Chunk of text",
    metadata={"chunk_id": 1}
)
```

### Node ê´€ê³„

```python
from llama_index.core.schema import NodeRelationship, RelatedNodeInfo

# Child -> Parent ê´€ê³„
child_node.relationships[NodeRelationship.PARENT] = RelatedNodeInfo(
    node_id=parent_node.node_id
)

# Parent -> Children ê´€ê³„
parent_node.relationships[NodeRelationship.CHILD] = [
    RelatedNodeInfo(node_id=child.node_id) for child in children
]
```

### Index

ê²€ìƒ‰ ê°€ëŠ¥í•œ ë°ì´í„° êµ¬ì¡°ì…ë‹ˆë‹¤.

```python
from llama_index.core import VectorStoreIndex

index = VectorStoreIndex.from_documents([doc1, doc2])
```

### Query Engine

ì¸ë±ìŠ¤ì— ëŒ€í•œ ì§ˆì˜ë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤.

```python
query_engine = index.as_query_engine()
response = query_engine.query("What is LlamaIndex?")
```

---

## 4. ì¸ë±ìŠ¤ íƒ€ì…

### 1. VectorStoreIndex

**ì„ë² ë”© ê¸°ë°˜ ì˜ë¯¸ ê²€ìƒ‰**

```python
index = VectorStoreIndex.from_documents(documents)
```

- **ì–¸ì œ ì‚¬ìš©?**: ì˜ë¯¸ ê¸°ë°˜ ê²€ìƒ‰, ìœ ì‚¬ ë¬¸ì„œ ì°¾ê¸°
- **ì¥ì **: ì •í™•í•œ ì˜ë¯¸ ë§¤ì¹­
- **ë‹¨ì **: ì„ë² ë”© ìƒì„± ë¹„ìš©

### 2. SummaryIndex

**ëª¨ë“  ë…¸ë“œë¥¼ ìˆœíšŒí•˜ë©° ìš”ì•½ ìƒì„±**

```python
index = SummaryIndex.from_documents(documents)
```

- **ì–¸ì œ ì‚¬ìš©?**: ì „ì²´ ë¬¸ì„œ ìš”ì•½, ê°œìš” íŒŒì•…
- **ì¥ì **: ëª¨ë“  ì •ë³´ í™œìš©
- **ë‹¨ì **: ëŠë¦¼, ë¹„ìš© ë†’ìŒ

### 3. TreeIndex

**íŠ¸ë¦¬ êµ¬ì¡°ë¡œ ê³„ì¸µì  ìš”ì•½**

```python
index = TreeIndex.from_documents(documents)
```

- **ì–¸ì œ ì‚¬ìš©?**: ê³„ì¸µì  ìš”ì•½, Top-down ê²€ìƒ‰
- **ì¥ì **: íš¨ìœ¨ì ì¸ ìš”ì•½
- **ë‹¨ì **: êµ¬ì¡° ìƒì„± ë³µì¡

### 4. KeywordTableIndex

**í‚¤ì›Œë“œ ê¸°ë°˜ ê²€ìƒ‰**

```python
index = KeywordTableIndex.from_documents(documents)
```

- **ì–¸ì œ ì‚¬ìš©?**: ì •í™•í•œ í‚¤ì›Œë“œ ë§¤ì¹­
- **ì¥ì **: ë¹ ë¦„, ì •í™•í•œ ë‹¨ì–´ ë§¤ì¹­
- **ë‹¨ì **: ì˜ë¯¸ ê²€ìƒ‰ ë¶ˆê°€

---

## 5. ì‹¤ìŠµ ì˜ˆì œ

### ì˜ˆì œ 1: ê¸°ë³¸ ë²¡í„° ì¸ë±ì‹±

```python
from llama_index.core import Document, VectorStoreIndex, Settings
from llama_index.llms.openai import OpenAI
from llama_index.embeddings.openai import OpenAIEmbedding

# ì„¤ì •
Settings.llm = OpenAI(model="gpt-4o-mini")
Settings.embed_model = OpenAIEmbedding(model="text-embedding-3-small")

# ë¬¸ì„œ ìƒì„±
doc = Document(text="LlamaIndex is a data framework for LLM applications.")

# ì¸ë±ìŠ¤ ìƒì„±
index = VectorStoreIndex.from_documents([doc])

# ì¿¼ë¦¬
query_engine = index.as_query_engine()
response = query_engine.query("What is LlamaIndex?")
print(response)
```

### ì˜ˆì œ 2: ê³„ì¸µì  ë…¸ë“œ íŒŒì‹±

```python
from llama_index.core.node_parser import HierarchicalNodeParser

# ê³„ì¸µì  íŒŒì„œ (3 ë ˆë²¨)
node_parser = HierarchicalNodeParser.from_defaults(
    chunk_sizes=[2048, 512, 128]  # í° ì²­í¬ -> ì¤‘ê°„ -> ì‘ì€ ì²­í¬
)

# ë…¸ë“œ íŒŒì‹±
nodes = node_parser.get_nodes_from_documents([long_document])
leaf_nodes = node_parser.get_leaf_nodes(nodes)

# Leaf ë…¸ë“œë¡œ ì¸ë±ìŠ¤ ìƒì„±
index = VectorStoreIndex(leaf_nodes)
```

**ê³„ì¸µ êµ¬ì¡°:**

```
Level 1: 2048ì ì²­í¬ (ì„¹ì…˜ ë ˆë²¨)
  â””â”€ Level 2: 512ì ì²­í¬ (ë‹¨ë½ ë ˆë²¨)
      â””â”€ Level 3: 128ì ì²­í¬ (ë¬¸ì¥ ë ˆë²¨)
```

### ì˜ˆì œ 3: JSON ë¬¸ì„œ ì¸ë±ì‹±

```python
import json
from llama_index.core.schema import TextNode

# JSON ë°ì´í„°
json_data = {
    "product": {
        "name": "Laptop",
        "specs": {
            "cpu": "Intel i7",
            "ram": "16GB"
        }
    }
}

# í‰íƒ„í™” í•¨ìˆ˜
def flatten_json(data, prefix=""):
    texts = []
    if isinstance(data, dict):
        for key, value in data.items():
            new_prefix = f"{prefix}.{key}" if prefix else key
            if isinstance(value, (dict, list)):
                texts.extend(flatten_json(value, new_prefix))
            else:
                texts.append(f"{new_prefix}: {value}")
    elif isinstance(data, list):
        for i, item in enumerate(data):
            texts.extend(flatten_json(item, f"{prefix}[{i}]"))
    return texts

# ë…¸ë“œ ìƒì„±
text_lines = flatten_json(json_data)
nodes = [TextNode(text=line) for line in text_lines]

# ì¸ë±ìŠ¤
index = VectorStoreIndex(nodes)
```

**ê²°ê³¼:**

```
product.name: Laptop
product.specs.cpu: Intel i7
product.specs.ram: 16GB
```

### ì˜ˆì œ 4: í…Œì´ë¸” ë°ì´í„° ì¸ë±ì‹±

```python
import pandas as pd
from llama_index.core.schema import TextNode

# CSV ë¡œë“œ
df = pd.read_csv("employees.csv")

# ê° í–‰ì„ ë…¸ë“œë¡œ
row_nodes = []
for idx, row in df.iterrows():
    text = " | ".join([f"{col}: {row[col]}" for col in df.columns])
    node = TextNode(text=text, metadata={"row": idx})
    row_nodes.append(node)

# ì—´ í†µê³„ ë…¸ë“œ
column_nodes = []
for col in df.columns:
    if df[col].dtype in ['int64', 'float64']:
        stats = df[col].describe()
        text = f"{col}: mean={stats['mean']}, min={stats['min']}, max={stats['max']}"
    else:
        text = f"{col}: {', '.join(df[col].unique())}"
    column_nodes.append(TextNode(text=text, metadata={"column": col}))

# ì¸ë±ìŠ¤ ìƒì„±
all_nodes = row_nodes + column_nodes
index = VectorStoreIndex(all_nodes)
```

### ì˜ˆì œ 5: Router Query Engine

```python
from llama_index.core.query_engine import RouterQueryEngine
from llama_index.core.selectors import LLMSingleSelector
from llama_index.core.tools import QueryEngineTool, ToolMetadata

# ì—¬ëŸ¬ ì¸ë±ìŠ¤ ìƒì„±
vector_index = VectorStoreIndex.from_documents(docs)
summary_index = SummaryIndex.from_documents(docs)
keyword_index = KeywordTableIndex.from_documents(docs)

# Tools ì •ì˜
vector_tool = QueryEngineTool(
    query_engine=vector_index.as_query_engine(),
    metadata=ToolMetadata(
        name="vector_search",
        description="Use for semantic search"
    )
)

summary_tool = QueryEngineTool(
    query_engine=summary_index.as_query_engine(),
    metadata=ToolMetadata(
        name="summary",
        description="Use for summaries"
    )
)

keyword_tool = QueryEngineTool(
    query_engine=keyword_index.as_query_engine(),
    metadata=ToolMetadata(
        name="keyword",
        description="Use for keyword search"
    )
)

# Router ìƒì„±
router = RouterQueryEngine(
    selector=LLMSingleSelector.from_defaults(),
    query_engine_tools=[vector_tool, summary_tool, keyword_tool]
)

# ìë™ ì„ íƒ
response = router.query("What is the main topic?")  # â†’ summary_tool
response = router.query("Find documents about AI")  # â†’ vector_tool
```

### ì˜ˆì œ 6: ì»¤ìŠ¤í…€ Parent-Child ë…¸ë“œ

```python
from llama_index.core.schema import NodeRelationship, RelatedNodeInfo
from llama_index.core.node_parser import SentenceSplitter

# Parent ë…¸ë“œ
parent_node = TextNode(
    text="This is a long section of text...",
    metadata={"type": "parent"}
)

# Child ë…¸ë“œ ìƒì„±
splitter = SentenceSplitter(chunk_size=200)
child_texts = splitter.split_text(parent_node.text)

child_nodes = []
for i, text in enumerate(child_texts):
    child = TextNode(
        text=text,
        metadata={"type": "child", "chunk": i}
    )

    # ê´€ê³„ ì„¤ì •
    child.relationships[NodeRelationship.PARENT] = RelatedNodeInfo(
        node_id=parent_node.node_id
    )
    child_nodes.append(child)

# Parentì— Children ì—°ê²°
parent_node.relationships[NodeRelationship.CHILD] = [
    RelatedNodeInfo(node_id=child.node_id) for child in child_nodes
]

# Childë§Œìœ¼ë¡œ ì¸ë±ìŠ¤ (ê²€ìƒ‰ì€ ì‘ì€ ì²­í¬ë¡œ, ì»¨í…ìŠ¤íŠ¸ëŠ” ë¶€ëª¨ì—ì„œ)
index = VectorStoreIndex(child_nodes)
```

### ì˜ˆì œ 7: Recursive Retriever

```python
from llama_index.core.retrievers import RecursiveRetriever
from llama_index.core.query_engine import RetrieverQueryEngine

# ê³„ì¸µì  ì¸ë±ìŠ¤ (ì˜ˆì œ 2ì˜ ê²°ê³¼ ì‚¬ìš©)
# nodes: ëª¨ë“  ë…¸ë“œ, leaf_nodes: ë¦¬í”„ ë…¸ë“œë§Œ

# node_id -> node ë§¤í•‘
node_dict = {node.node_id: node for node in nodes}

# Recursive Retriever
retriever = RecursiveRetriever(
    "vector",
    retriever_dict={"vector": index.as_retriever()},
    node_dict=node_dict
)

# Query Engine
query_engine = RetrieverQueryEngine.from_args(retriever)

# ì¿¼ë¦¬ (ì‘ì€ ì²­í¬ë¡œ ê²€ìƒ‰ â†’ ë¶€ëª¨ ì»¨í…ìŠ¤íŠ¸ ìë™ ë¡œë“œ)
response = query_engine.query("What is machine learning?")
```

---

## 6. FastAPI ì—”ë“œí¬ì¸íŠ¸

### ì„œë²„ ì‹œì‘

```bash
poetry install
poetry run start
```

Swagger UI: http://localhost:8001/docs

### ì—”ë“œí¬ì¸íŠ¸ ëª©ë¡

| ì—”ë“œí¬ì¸íŠ¸ | ì„¤ëª… |
|----------|------|
| `POST /llamaindex/basic-vector-index` | ê¸°ë³¸ ë²¡í„° ì¸ë±ìŠ¤ ìƒì„± |
| `POST /llamaindex/query-vector-index` | ë²¡í„° ì¸ë±ìŠ¤ ì¿¼ë¦¬ |
| `POST /llamaindex/hierarchical-index` | ê³„ì¸µì  ì¸ë±ìŠ¤ ìƒì„± |
| `POST /llamaindex/json-index` | JSON ë¬¸ì„œ ì¸ë±ì‹± |
| `POST /llamaindex/table-index` | í…Œì´ë¸” ë°ì´í„° ì¸ë±ì‹± |
| `POST /llamaindex/multi-index` | ë‹¤ì¤‘ ì¸ë±ìŠ¤ + Router |
| `POST /llamaindex/query-router` | Routerë¡œ ì¿¼ë¦¬ |
| `POST /llamaindex/recursive-retriever` | ì¬ê·€ì  ê²€ìƒ‰ |
| `POST /llamaindex/custom-nodes` | ì»¤ìŠ¤í…€ ë…¸ë“œ ìƒì„± |
| `GET /llamaindex/list-indices` | ì¸ë±ìŠ¤ ëª©ë¡ |
| `DELETE /llamaindex/delete-index/{id}` | ì¸ë±ìŠ¤ ì‚­ì œ |

### ì‚¬ìš© ì˜ˆì‹œ

#### 1. ê¸°ë³¸ ë²¡í„° ì¸ë±ìŠ¤ ìƒì„±

```bash
curl -X POST "http://localhost:8001/llamaindex/basic-vector-index" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "LlamaIndex is a data framework for LLM applications.",
    "doc_id": "doc1"
  }'
```

#### 2. ì¿¼ë¦¬ ì‹¤í–‰

```bash
curl -X POST "http://localhost:8001/llamaindex/query-vector-index" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "What is LlamaIndex?",
    "index_id": "vector_doc1"
  }'
```

---

## 7. ê³ ê¸‰ íŒ¨í„´

### íŒ¨í„´ 1: ë©”íƒ€ë°ì´í„° í•„í„°ë§

```python
from llama_index.core.vector_stores import MetadataFilters, ExactMatchFilter

# íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ ê²€ìƒ‰
filtered_query_engine = index.as_query_engine(
    filters=MetadataFilters(
        filters=[ExactMatchFilter(key="category", value="ai")]
    )
)

response = filtered_query_engine.query("Tell me about AI")
```

### íŒ¨í„´ 2: í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰

```python
from llama_index.core.retrievers import VectorIndexRetriever, KeywordTableRetriever
from llama_index.core.retrievers import QueryFusionRetriever

# ë²¡í„° + í‚¤ì›Œë“œ ê²°í•©
vector_retriever = VectorIndexRetriever(index=vector_index)
keyword_retriever = KeywordTableRetriever(index=keyword_index)

fusion_retriever = QueryFusionRetriever(
    [vector_retriever, keyword_retriever],
    similarity_top_k=5
)

response = fusion_retriever.retrieve("AI and machine learning")
```

### íŒ¨í„´ 3: Re-ranking

```python
from llama_index.core.postprocessor import SimilarityPostprocessor

# ìœ ì‚¬ë„ ê¸°ì¤€ìœ¼ë¡œ ì¬ì •ë ¬
postprocessor = SimilarityPostprocessor(similarity_cutoff=0.7)

query_engine = index.as_query_engine(
    node_postprocessors=[postprocessor]
)
```

### íŒ¨í„´ 4: ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ

```python
# ìŠ¤íŠ¸ë¦¬ë° ì¿¼ë¦¬
streaming_response = query_engine.query("Explain AI")

for text in streaming_response.response_gen:
    print(text, end="")
```

---

## 8. ì„±ëŠ¥ ìµœì í™”

### 1. ì²­í¬ í¬ê¸° ìµœì í™”

```python
# ì‘ì€ ì²­í¬: ì •í™•í•˜ì§€ë§Œ ëŠë¦¼
Settings.chunk_size = 256

# í° ì²­í¬: ë¹ ë¥´ì§€ë§Œ ëœ ì •í™•
Settings.chunk_size = 1024

# ì¶”ì²œ: 512 (ê· í˜•)
Settings.chunk_size = 512
```

### 2. Top-K ì¡°ì •

```python
# ë” ë§ì€ ê²°ê³¼ (ì •í™•í•˜ì§€ë§Œ ëŠë¦¼)
query_engine = index.as_query_engine(similarity_top_k=10)

# ì ì€ ê²°ê³¼ (ë¹ ë¥´ì§€ë§Œ ëœ ì •í™•)
query_engine = index.as_query_engine(similarity_top_k=3)
```

### 3. ìºì‹±

```python
from llama_index.core.storage.storage_context import StorageContext

# ì¸ë±ìŠ¤ ì €ì¥
storage_context = StorageContext.from_defaults()
index.storage_context = storage_context
index.storage_context.persist(persist_dir="./storage")

# ì¸ë±ìŠ¤ ë¡œë“œ
from llama_index.core import load_index_from_storage

storage_context = StorageContext.from_defaults(persist_dir="./storage")
index = load_index_from_storage(storage_context)
```

### 4. ë°°ì¹˜ ì²˜ë¦¬

```python
# ì—¬ëŸ¬ ë¬¸ì„œ í•œ ë²ˆì— ì¸ë±ì‹±
documents = [doc1, doc2, doc3, ...]  # 100ê°œ
index = VectorStoreIndex.from_documents(documents)  # í•œ ë²ˆì—
```

---

## 9. ë¬¸ì œ í•´ê²°

### ë¬¸ì œ 1: ì„ë² ë”© API ë¹„ìš©ì´ ë„ˆë¬´ ë†’ìŒ

**í•´ê²°ì±…:**

```python
# 1. ë¡œì»¬ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
from llama_index.embeddings.huggingface import HuggingFaceEmbedding

Settings.embed_model = HuggingFaceEmbedding(
    model_name="BAAI/bge-small-en-v1.5"
)

# 2. ì²­í¬ í¬ê¸° ì¦ê°€
Settings.chunk_size = 1024

# 3. ìºì‹± í™œìš©
```

### ë¬¸ì œ 2: ê²€ìƒ‰ ê²°ê³¼ê°€ ë¶€ì •í™•í•¨

**í•´ê²°ì±…:**

```python
# 1. ì²­í¬ í¬ê¸° ê°ì†Œ
Settings.chunk_size = 256

# 2. Top-K ì¦ê°€
query_engine = index.as_query_engine(similarity_top_k=10)

# 3. Re-ranking ì¶”ê°€
from llama_index.core.postprocessor import SimilarityPostprocessor
postprocessor = SimilarityPostprocessor(similarity_cutoff=0.75)
query_engine = index.as_query_engine(node_postprocessors=[postprocessor])
```

### ë¬¸ì œ 3: ë©”ëª¨ë¦¬ ë¶€ì¡±

**í•´ê²°ì±…:**

```python
# 1. ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
for doc in large_doc_iterator:
    nodes = parser.get_nodes_from_documents([doc])
    index.insert_nodes(nodes)

# 2. ë°°ì¹˜ í¬ê¸° ì œí•œ
batch_size = 100
for i in range(0, len(docs), batch_size):
    batch = docs[i:i+batch_size]
    # ì²˜ë¦¬
```

### ë¬¸ì œ 4: ì¿¼ë¦¬ê°€ ë„ˆë¬´ ëŠë¦¼

**í•´ê²°ì±…:**

```python
# 1. ì¸ë±ìŠ¤ íƒ€ì… ë³€ê²½ (Vector â†’ Keyword)
keyword_index = KeywordTableIndex.from_documents(docs)

# 2. ìºì‹±
# (ìœ„ì˜ ìºì‹± ì„¹ì…˜ ì°¸ì¡°)

# 3. ë¹„ë™ê¸° ì¿¼ë¦¬
response = await query_engine.aquery("question")
```

---

## ğŸ¯ í•™ìŠµ ë¡œë“œë§µ

### ì´ˆê¸‰ (1ì£¼)
1. âœ… ê¸°ë³¸ ë²¡í„° ì¸ë±ìŠ¤ ìƒì„± ë° ì¿¼ë¦¬
2. âœ… ë©”íƒ€ë°ì´í„° í™œìš©
3. âœ… ê°„ë‹¨í•œ JSON/CSV ì¸ë±ì‹±

### ì¤‘ê¸‰ (2ì£¼)
4. âœ… ê³„ì¸µì  ë…¸ë“œ íŒŒì‹±
5. âœ… Router Query Engine
6. âœ… ì»¤ìŠ¤í…€ ë…¸ë“œ ê´€ê³„

### ê³ ê¸‰ (3ì£¼)
7. âœ… Recursive Retriever
8. âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
9. âœ… ì„±ëŠ¥ ìµœì í™”
10. âœ… í”„ë¡œë•ì…˜ ë°°í¬

---

## ğŸ“– ì¶”ê°€ í•™ìŠµ ìë£Œ

- [LlamaIndex ê³µì‹ ë¬¸ì„œ](https://docs.llamaindex.ai/)
- [LlamaIndex GitHub](https://github.com/run-llama/llama_index)
- [LlamaIndex Discord](https://discord.gg/dGcwcsnxhU)

---

## ğŸ”§ ìœ ìš©í•œ ë„êµ¬

### 1. LlamaHub

ë‹¤ì–‘í•œ ë°ì´í„° ë¡œë” ëª¨ìŒ

```python
from llama_index.core import download_loader

PDFReader = download_loader("PDFReader")
docs = PDFReader().load_data("document.pdf")
```

### 2. LlamaIndex CLI

```bash
# ì¸ë±ìŠ¤ ìƒì„±
llamaindex-cli create-index --data-dir ./data

# ì¿¼ë¦¬
llamaindex-cli query "What is AI?"
```

### 3. Observability

```python
import llama_index.core
llama_index.core.set_global_handler("simple")

# ë””ë²„ê·¸ ì •ë³´ ìë™ ì¶œë ¥
```

---

**ì‘ì„±ì¼**: 2024-01-14
**ë²„ì „**: 1.0
**ë¬¸ì˜**: hyunbae.jeon@example.com
